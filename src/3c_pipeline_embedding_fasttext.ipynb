{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Neuronales\n",
    "\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importanción de librería requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE_READ = 'docs/preprocessing_reddit_data.csv'\n",
    "TEXT_SAVE_FILE = 'docs/reddit_data_fasttext.csv'\n",
    "FILENAME_PICKLE = \"docs/tmpreddit.pickle\"\n",
    "n_clusters = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los comentarios de Reddit\n",
    "\n",
    "Los comentarios fueron previamente preprocesados (Ver en TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(FILENAME_PICKLE, 'rb') as f:\n",
    "    df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['lemma_tokens'])\n",
    "\n",
    "# Filtering Extremes\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "\n",
    "# Creating a corpus object\n",
    "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]\n",
    "\n",
    "processed_corpus = df['lemma_tokens']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18381159, 18695800)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(processed_corpus, total_examples=len(processed_corpus), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algunas predicciones\n",
    "model.get_nearest_neighbors('peron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9955493211746216, 'cristi'),\n",
       " (0.9921181797981262, 'kirchnerista'),\n",
       " (0.9887705445289612, 'cristina_fernandez'),\n",
       " (0.9884024858474731, 'cristina_fernández'),\n",
       " (0.988014280796051, 'cristín'),\n",
       " (0.9856591820716858, 'kirchnerismo'),\n",
       " (0.9838628172874451, 'fernández'),\n",
       " (0.9806126952171326, 'argentino\"'),\n",
       " (0.9791547060012817, 'turista'),\n",
       " (0.9788342118263245, 'ladrona')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('cristina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9962456822395325, 'cfk'),\n",
       " (0.9956827163696289, 'lopez'),\n",
       " (0.9956071376800537, 'seinfeld'),\n",
       " (0.9955170154571533, 'zombie'),\n",
       " (0.9954903721809387, 'remake'),\n",
       " (0.9953209161758423, 'bullrich'),\n",
       " (0.9948703646659851, 'stealth'),\n",
       " (0.9945695400238037, 'users'),\n",
       " (0.9945260286331177, 'stewart'),\n",
       " (0.9945029616355896, 'evil')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('nestor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9928846955299377, 'economy'),\n",
       " (0.9909766316413879, 'neoliberalismo'),\n",
       " (0.9904403686523438, 'nací'),\n",
       " (0.9893696904182434, 'republica_argentina'),\n",
       " (0.9881485104560852, 'ladron'),\n",
       " (0.9877192378044128, 'fernandez'),\n",
       " (0.9875622391700745, 'crea'),\n",
       " (0.986701488494873, 'arg'),\n",
       " (0.9864534735679626, 'referis'),\n",
       " (0.9864226579666138, 'cereal')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('neoliberal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9982995390892029, 'doctrina'),\n",
       " (0.9981448650360107, 'petróleo'),\n",
       " (0.9979062676429749, 'turbina'),\n",
       " (0.9976403117179871, 'juez'),\n",
       " (0.9974097609519958, 'marihuán'),\n",
       " (0.9973348379135132, 'gris'),\n",
       " (0.9971646070480347, 'nena'),\n",
       " (0.9970555901527405, 'wanda_nara'),\n",
       " (0.9969688057899475, 'gorila'),\n",
       " (0.9969642758369446, 'película')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('malvinas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9841022491455078, 'malvina'),\n",
       " (0.9838746190071106, 'títerir'),\n",
       " (0.9836748242378235, 'dona'),\n",
       " (0.9829391837120056, 'malvinas'),\n",
       " (0.9824694395065308, 'doctrina'),\n",
       " (0.9824132919311523, 'estatua'),\n",
       " (0.9818088412284851, 'reina'),\n",
       " (0.9814509153366089, 'infarto'),\n",
       " (0.9808228015899658, 'estafa'),\n",
       " (0.980640709400177, 'archivo')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_analogies('cristina', 'ladrona', 'alberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mapuchez', 0.975992739200592),\n",
       " ('mapuches', 0.9645689725875854),\n",
       " ('mapuchecoin', 0.9489633440971375),\n",
       " ('mapuch', 0.9372188448905945),\n",
       " ('no-mapuches', 0.9177072048187256),\n",
       " ('pseudomapuche', 0.9131770133972168),\n",
       " ('mapu', 0.8864879012107849),\n",
       " ('mapuchir', 0.8846395611763),\n",
       " ('pseudo-mapuches', 0.8769819736480713),\n",
       " ('pseudomapuch', 0.8387230038642883)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('mapuche')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de vectores desde documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27791, 100)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "    \n",
    "vectorized_docs = vectorize(processed_corpus, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 70\n",
      "Silhouette coefficient: 0.01\n",
      "Inertia:1635970.9632375669\n",
      "Silhouette values:\n",
      "    Cluster 13: Size:223 | Avg:0.22 | Min:0.07 | Max: 0.38\n",
      "    Cluster 49: Size:156 | Avg:0.14 | Min:-0.05 | Max: 0.31\n",
      "    Cluster 67: Size:53 | Avg:0.11 | Min:-0.07 | Max: 0.34\n",
      "    Cluster 11: Size:349 | Avg:0.11 | Min:-0.05 | Max: 0.27\n",
      "    Cluster 28: Size:236 | Avg:0.09 | Min:-0.06 | Max: 0.26\n",
      "    Cluster 21: Size:139 | Avg:0.09 | Min:-0.07 | Max: 0.27\n",
      "    Cluster 42: Size:90 | Avg:0.09 | Min:-0.08 | Max: 0.30\n",
      "    Cluster 26: Size:189 | Avg:0.08 | Min:-0.08 | Max: 0.29\n",
      "    Cluster 63: Size:303 | Avg:0.08 | Min:-0.05 | Max: 0.26\n",
      "    Cluster 27: Size:90 | Avg:0.07 | Min:-0.06 | Max: 0.26\n",
      "    Cluster 45: Size:207 | Avg:0.07 | Min:-0.09 | Max: 0.26\n",
      "    Cluster 23: Size:207 | Avg:0.07 | Min:-0.07 | Max: 0.28\n",
      "    Cluster 34: Size:92 | Avg:0.07 | Min:-0.05 | Max: 0.23\n",
      "    Cluster 33: Size:256 | Avg:0.06 | Min:-0.08 | Max: 0.24\n",
      "    Cluster 38: Size:231 | Avg:0.05 | Min:-0.07 | Max: 0.26\n",
      "    Cluster 20: Size:1648 | Avg:0.05 | Min:0.01 | Max: 0.11\n",
      "    Cluster 10: Size:178 | Avg:0.05 | Min:-0.07 | Max: 0.23\n",
      "    Cluster 58: Size:201 | Avg:0.05 | Min:-0.08 | Max: 0.25\n",
      "    Cluster 50: Size:358 | Avg:0.05 | Min:-0.14 | Max: 0.17\n",
      "    Cluster 66: Size:371 | Avg:0.05 | Min:-0.07 | Max: 0.23\n",
      "    Cluster 64: Size:272 | Avg:0.05 | Min:-0.09 | Max: 0.21\n",
      "    Cluster 14: Size:112 | Avg:0.04 | Min:-0.14 | Max: 0.27\n",
      "    Cluster 59: Size:160 | Avg:0.04 | Min:-0.10 | Max: 0.23\n",
      "    Cluster 48: Size:123 | Avg:0.04 | Min:-0.08 | Max: 0.26\n",
      "    Cluster 5: Size:387 | Avg:0.03 | Min:-0.08 | Max: 0.19\n",
      "    Cluster 65: Size:800 | Avg:0.03 | Min:-0.04 | Max: 0.18\n",
      "    Cluster 25: Size:468 | Avg:0.03 | Min:-0.11 | Max: 0.25\n",
      "    Cluster 43: Size:195 | Avg:0.03 | Min:-0.10 | Max: 0.21\n",
      "    Cluster 22: Size:958 | Avg:0.03 | Min:-0.02 | Max: 0.08\n",
      "    Cluster 40: Size:256 | Avg:0.03 | Min:-0.10 | Max: 0.21\n",
      "    Cluster 47: Size:93 | Avg:0.03 | Min:-0.12 | Max: 0.24\n",
      "    Cluster 19: Size:157 | Avg:0.02 | Min:-0.13 | Max: 0.24\n",
      "    Cluster 53: Size:284 | Avg:0.02 | Min:-0.08 | Max: 0.19\n",
      "    Cluster 9: Size:347 | Avg:0.02 | Min:-0.13 | Max: 0.20\n",
      "    Cluster 52: Size:285 | Avg:0.01 | Min:-0.11 | Max: 0.18\n",
      "    Cluster 24: Size:428 | Avg:0.01 | Min:-0.11 | Max: 0.22\n",
      "    Cluster 69: Size:146 | Avg:0.01 | Min:-0.15 | Max: 0.23\n",
      "    Cluster 12: Size:996 | Avg:0.01 | Min:-0.04 | Max: 0.08\n",
      "    Cluster 62: Size:434 | Avg:0.00 | Min:-0.11 | Max: 0.19\n",
      "    Cluster 7: Size:685 | Avg:-0.00 | Min:-0.07 | Max: 0.11\n",
      "    Cluster 18: Size:1480 | Avg:-0.01 | Min:-0.07 | Max: 0.07\n",
      "    Cluster 30: Size:128 | Avg:-0.01 | Min:-0.14 | Max: 0.16\n",
      "    Cluster 16: Size:1236 | Avg:-0.01 | Min:-0.07 | Max: 0.07\n",
      "    Cluster 54: Size:319 | Avg:-0.01 | Min:-0.12 | Max: 0.15\n",
      "    Cluster 55: Size:349 | Avg:-0.01 | Min:-0.12 | Max: 0.15\n",
      "    Cluster 56: Size:250 | Avg:-0.01 | Min:-0.15 | Max: 0.14\n",
      "    Cluster 29: Size:519 | Avg:-0.01 | Min:-0.13 | Max: 0.14\n",
      "    Cluster 57: Size:711 | Avg:-0.02 | Min:-0.08 | Max: 0.08\n",
      "    Cluster 36: Size:570 | Avg:-0.02 | Min:-0.13 | Max: 0.15\n",
      "    Cluster 61: Size:762 | Avg:-0.02 | Min:-0.10 | Max: 0.10\n",
      "    Cluster 32: Size:237 | Avg:-0.02 | Min:-0.15 | Max: 0.13\n",
      "    Cluster 46: Size:1016 | Avg:-0.02 | Min:-0.10 | Max: 0.08\n",
      "    Cluster 51: Size:242 | Avg:-0.02 | Min:-0.14 | Max: 0.19\n",
      "    Cluster 3: Size:429 | Avg:-0.02 | Min:-0.13 | Max: 0.15\n",
      "    Cluster 68: Size:285 | Avg:-0.02 | Min:-0.10 | Max: 0.11\n",
      "    Cluster 31: Size:381 | Avg:-0.03 | Min:-0.14 | Max: 0.15\n",
      "    Cluster 39: Size:922 | Avg:-0.03 | Min:-0.10 | Max: 0.09\n",
      "    Cluster 8: Size:888 | Avg:-0.03 | Min:-0.12 | Max: 0.09\n",
      "    Cluster 17: Size:191 | Avg:-0.03 | Min:-0.16 | Max: 0.15\n",
      "    Cluster 41: Size:137 | Avg:-0.03 | Min:-0.13 | Max: 0.14\n",
      "    Cluster 15: Size:227 | Avg:-0.03 | Min:-0.14 | Max: 0.14\n",
      "    Cluster 6: Size:789 | Avg:-0.04 | Min:-0.13 | Max: 0.10\n",
      "    Cluster 44: Size:120 | Avg:-0.04 | Min:-0.12 | Max: 0.10\n",
      "    Cluster 2: Size:462 | Avg:-0.04 | Min:-0.11 | Max: 0.09\n",
      "    Cluster 0: Size:239 | Avg:-0.04 | Min:-0.15 | Max: 0.10\n",
      "    Cluster 37: Size:181 | Avg:-0.05 | Min:-0.19 | Max: 0.13\n",
      "    Cluster 1: Size:528 | Avg:-0.05 | Min:-0.15 | Max: 0.07\n",
      "    Cluster 60: Size:373 | Avg:-0.05 | Min:-0.19 | Max: 0.14\n",
      "    Cluster 35: Size:247 | Avg:-0.05 | Min:-0.17 | Max: 0.11\n",
      "    Cluster 4: Size:410 | Avg:-0.06 | Min:-0.16 | Max: 0.10\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=n_clusters,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": df[\"body\"].values,\n",
    "    \"tokens\": [\" \".join(text) for text in processed_corpus],\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Top terms* por cluster (basado en los centroides de los clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: macristo macri~~. macris macrium macri-peño macri mracri macrismo macristaaaa -macri \n",
      "Cluster 1: desgraciada desgracia felizmente \\*gracia gracia desgraciado agradecida graciassss graciasss desagraciado \n",
      "Cluster 2: perpetúar perfumar permítanmar perimetro perpetuar permanente perturbadoras percatar perpetrado perturbador \n",
      "Cluster 3: viejo abejo cejo vieja.me.dejo viejo= pendeviejo viviente rejo festejo vieja \n",
      "Cluster 4: rindo festilindo lindo gondo brindo cundo hundo oriundo swgundo trotamundo \n",
      "Cluster 5: argentina?.\"al argentina-perú argentiniar argentino argentinatm argentina1234 argentinans aegentino argentinoo argentinadepie \n",
      "Cluster 6: cavar caducó cat6 caús caliente cararrotar carvajal caroyar capua caído \n",
      "Cluster 7: desmadrar enterrar desenterrar yikar oar depurar tetrar descifrar delirar ríar \n",
      "Cluster 8: agua calientenla caliente agua\\ piojo aguja aguila freelo riachuelo pintamelo \n",
      "Cluster 9: pesos dólar peso dolar dólar_har ~~dolares~~ embolar bipolar amándolar indicándolar \n",
      "Cluster 10: hablar hablarl cablar hablarles hablarme hablaba ensamblar hablábar blar habla \n",
      "Cluster 11: vo -vo vox vofi vodka vodi voraz vos octavo votás \n",
      "Cluster 12: ineficiente deficiente injusticiar eficiente coeficiente intervencionista creciente combatiente derrotar intervencionismo \n",
      "Cluster 13: theory thought theyll th thank though without thanks third thots \n",
      "Cluster 14: mano el.mano mano(si rumano manos manor piromano manoplar lavamano romano \n",
      "Cluster 15: matar atar mametar jonatar maorretar johnatar anismanlomatar rematar fíjatar marmotar \n",
      "Cluster 16: laempresanoesresponsablepordesgraciaspersonalnisupersonalestargenuinamenteinteresadoenloqueocurraenlavidadelcliente administrativo ineficiente descentralización administrar regularmente industrialización reglamenteción conscriptar concientizar \n",
      "Cluster 17: faltar falt falta faltan faltariar faltant fal gibraltar asfaltar false \n",
      "Cluster 18: ⣰ ⠳ ⡦ ⡶ 00:02:20 ⠟ ⢗ ⠞ ⢘ ⠶ \n",
      "Cluster 19: hombre homre->mujer ~~hombre hombr mujer exmujer ~~nombre~~ mujeres nombre hombro \n",
      "Cluster 20: laempresanoesresponsablepordesgraciaspersonalnisupersonalestargenuinamenteinteresadoenloqueocurraenlavidadelcliente intraducible descentralizado inexistente intransigente ralentizado detergente autoadministrado ineficiente creciente \n",
      "Cluster 21: tén obtén té cén mágén sartén avén tengan mantén amén \n",
      "Cluster 22: terrateniente coeficiente terriblemente inocente corruptamente fehaciente ineficiente intermitente inconsciente consciente \n",
      "Cluster 23: dar odar rodar gaydar sudar dartagnar amoldar apodar leudar darl \n",
      "Cluster 24: jajaja jajajaja jajajaajaja jajajar jajajajar jajajaa jajajejox jajajaajj jajajaaj jajajajajar \n",
      "Cluster 25: ver aver veríar vancouver verl abrosver elver veré over sever \n",
      "Cluster 26: poner iponer ponerl oponer ponerlo disponer exponer reponer oner sooner \n",
      "Cluster 27: mes mesas mesías mesía meshi james times mesopotamica mesi mesa \n",
      "Cluster 28: yo hoyo qcyo joyo él yayo yokozuna anyo oclayo hoja \n",
      "Cluster 29: comeriia comí coml comité comecar comerme comib comés comercios comi \n",
      "Cluster 30: mirar \\*mirar admirar lirar elvirar girar juirar dirar sátirar quirar \n",
      "Cluster 31: léxico eico exótico cívico púbico gótico lúdico soico acústico psíquico \n",
      "Cluster 32: esperabar desesperar esperar 2)esperar espesar señal_esperar inesperado esperanzado assperar esperanzador \n",
      "Cluster 33: venir devenir veniar venirte cezinkoenir venie veni cenir venis venitir \n",
      "Cluster 34: idea ideapad ideal idem ide idearia idk dea idealizar idealización \n",
      "Cluster 35: \\-car fiacar orcar bacar yikar oar acar azkar cacar car \n",
      "Cluster 36: pagarar pagar pagartar cobraplar pagartelo \\*pagar pagarl pagarlo cobra prepagar \n",
      "Cluster 37: auto arauto ruto luto flauto yuto autoretrato incauto analauto moto \n",
      "Cluster 38: salir salúdalir sali saliva salio salimos saliar pasalir salia salía \n",
      "Cluster 39: sube subite automáticamente)._^(feedback montevideo subterráneo elclubdelosprogramadoreseneuropa encarecidamente págín fotográficamente publicación \n",
      "Cluster 40: tomar \\*tomar tomate tomarlo tomatelá tomatela retomar tomaría tomarno tomatelas \n",
      "Cluster 41: escuchar escucharar escucharlo escuchaba edcuchar escucha -cuchar escuchado escuchás escuchastar \n",
      "Cluster 42: hijo hijaeputa hijos hijodeputo hij hijota putamadre hijo_puta hijaput laputamadre \n",
      "Cluster 43: gustar degustar gustari frustar gustarir gustaba gustaria gustavo gusta asustar \n",
      "Cluster 44: empecinar empezar bezar determinar terminar exterminar empeorar germinar fulminar terminal \n",
      "Cluster 45: va va- va\\ vaa valía vas valdeszeva pod vaya cueva \n",
      "Cluster 46: laempresanoesresponsablepordesgraciaspersonalnisupersonalestargenuinamenteinteresadoenloqueocurraenlavidadelcliente conscientemente eficientemente ineficiente coeficiente deficiente eficiente creciente exigente subconsciente \n",
      "Cluster 47: ir iriir iir yyyir oir jóvir irir qcir cdoir tujir \n",
      "Cluster 48: quedar quedabar quedariar quedara quedamo queda quedado quedartir quedo quedate \n",
      "Cluster 49: re reabre rex reí repostre refiere rel rev reev reltih \n",
      "Cluster 50: thought though wouldn´t wouldnt its thank without theory we theyll \n",
      "Cluster 51: paso pasar pasé pasarl pasá pasamo pase pasás pases pasarás \n",
      "Cluster 52: milei-espert milei milei.y mileinial milei’s \\-[*espert espert espertma spert kazuspert \n",
      "Cluster 53: tener tenerla obtener \\-tener abstener tenembaum tenete detener retener tenenbaum \n",
      "Cluster 54: ado nómado vado naado rayado evado wado ahumado hado zanjado \n",
      "Cluster 55: extender entender encender desentender tender pretender bender ofender atender revender \n",
      "Cluster 56: votar votarar votarl voto votos votarlo votastar sigan_votar votaba votanso \n",
      "Cluster 57: trabanar varíar ganancio estanciero equilibrar financieramente equivaliar ganancias irregulares irregular \n",
      "Cluster 58: decir \\-decir decirno decibl deci decidansir decimir decidi decirtar decirmelo \n",
      "Cluster 59: acá acábo ac acv acné \\*sacá aca acaa acabé acabás \n",
      "Cluster 60: ley leer leí leé ley_etiquetar leeré ley_etiquetado leyends leen leerlo \n",
      "Cluster 61: anti-peronismo peronmacrismo anti-zurdismo peronismo absurdísmo stalinismo abismo imperialismo estalinismo ultraderecha-neoliberal-imperialista-cipaya-procia \n",
      "Cluster 62: tipo tipoel tips tipea zalipo logotipo prototipo pipo tipeo tip \n",
      "Cluster 63: país expaís pís pbís mís vís insurgente exigente inmigrantes+planeros+clientelismo pobrismo \n",
      "Cluster 64: precio precios_cuidado control_precio precipicio crecio preciosbardeado aprecio precie precisar precio-calidad \n",
      "Cluster 65: él cosechar obligadamente inocente obligatoriamente corruptamente intermitente derrochar intento correctamente \n",
      "Cluster 66: año añs año.~~ años ermitaño equilibrado secuestrado creciente electorado secuestrador \n",
      "Cluster 67: vo vox -vo vofi vodka vodi vos voraz voz llvo \n",
      "Cluster 68: función_gobierno gobiedno gobierno montogobierno función_gobiernir cobierno gobiernola anses-gobierno.com gobierne goberno \n",
      "Cluster 69: ah ^ah -ah tdah ahy ahii ~~ah \\-ah ahh macristaaaa \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(n_clusters):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=10)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Top terms* por cluster (basado en las palabras más frecuentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: macri(76) alberto(54) /s(40) cristina(28) kirchner(18) \n",
      "Cluster 1: vida(167) gracias(139) op(33) gracia(32) dios(23) \n",
      "Cluster 2: perro(91) llamar(82) andar(27) pensar(23) mandar(23) \n",
      "Cluster 3: viejo(226) hijo(58) vivir(39) morir(34) puta(33) \n",
      "Cluster 4: lindo(110) mundo(84) che(32) mierda(31) mierdo(25) \n",
      "Cluster 5: argentino(230) argentina(153) mundo(16) r(15) pasar(13) \n",
      "Cluster 6: cara(107) casa(101) caer(84) cagar(63) calle(57) \n",
      "Cluster 7: dejar(212) pasar(60) entrar(57) tirar(49) querer(30) \n",
      "Cluster 8: agua(88) calor(81) orto(45) ojo(41) aire(39) \n",
      "Cluster 9: peso(117) dolar(110) pesos(72) dólar(64) dólares(33) \n",
      "Cluster 10: hablar(181) dejar(8) cabeza(7) escuchar(7) cristina(6) \n",
      "Cluster 11: vo(365) decir(30) él(28) so(16) pasar(15) \n",
      "Cluster 12: gente(82) vivir(75) provincia(56) nacional(44) querer(41) \n",
      "Cluster 13: the(195) of(74) and(70) you(63) to(60) \n",
      "Cluster 14: mano(100) man(6) querer(5) humano(4) duro(4) \n",
      "Cluster 15: matar(84) robar(33) contar(15) nisman(10) evitar(9) \n",
      "Cluster 16: dato(53) público(45) él(43) social(41) pasar(41) \n",
      "Cluster 17: faltar(67) falta(47) favor(35) señor(19) respeto(10) \n",
      "Cluster 18: x200b(58) ️(50) >(35) don(31) san(25) \n",
      "Cluster 19: hombre(91) mujer(87) nombre(30) gris(7) serio(6) \n",
      "Cluster 20: decir(60) hacer(58) pensar(53) pedir(50) pasar(46) \n",
      "Cluster 21: tén(151) pod(9) arte(7) él(7) razon(7) \n",
      "Cluster 22: él(470) querer(153) amigo(110) hacer(71) seguro(48) \n",
      "Cluster 23: dar(201) él(67) vuelta(14) querer(13) gente(8) \n",
      "Cluster 24: jaja(87) jajaja(72) jajajar(54) jajajaja(46) jajaj(36) \n",
      "Cluster 25: ver(440) hacer(31) él(15) parecer(14) foto(14) \n",
      "Cluster 26: poner(188) él(15) pasar(6) azúcar(5) familia(4) \n",
      "Cluster 27: mes(99) año(6) él(6) mínimo(4) pagar(4) \n",
      "Cluster 28: yo(242) decir(21) ir(18) él(17) decimir(13) \n",
      "Cluster 29: comer(146) comida(53) comprar(36) come(35) carne(30) \n",
      "Cluster 30: mirar(76) tirar(35) raro(9) ojo(7) pasar(5) \n",
      "Cluster 31: único(69) rico(53) chico(30) político(21) asco(16) \n",
      "Cluster 32: esperar(117) estudiar(34) clase(12) carrera(9) estudio(7) \n",
      "Cluster 33: venir(246) año(16) semana(13) mes(10) pasar(7) \n",
      "Cluster 34: idea(95) persona(6) pasar(5) jaja(4) liberal(4) \n",
      "Cluster 35: sacar(70) car(61) buscar(39) par(14) él(10) \n",
      "Cluster 36: pagar(222) impuesto(104) plata(59) plan(55) cobrar(49) \n",
      "Cluster 37: auto(117) foto(20) moto(18) punto(11) pasar(11) \n",
      "Cluster 38: salir(204) sal(14) correr(10) calle(9) salio(7) \n",
      "Cluster 39: foto(132) video(77) dato(42) alto(41) sub(40) \n",
      "Cluster 40: tomar(177) mate(66) malo(18) dulce(14) gente(12) \n",
      "Cluster 41: escuchar(72) música(17) conocer(10) voz(9) garchar(8) \n",
      "Cluster 42: hijo(65) puta(45) puto(12) madre(7) remil(5) \n",
      "Cluster 43: gustar(146) gusto(27) gusta(21) gente(6) fernet(6) \n",
      "Cluster 44: empezar(69) terminar(38) jugar(9) querer(8) serie(6) \n",
      "Cluster 45: va(203) pod(21) él(11) vo(9) tén(9) \n",
      "Cluster 46: gente(227) persona(129) tema(101) problema(72) pensar(66) \n",
      "Cluster 47: ir(73) él(8) decir(6) pasar(6) querer(5) \n",
      "Cluster 48: quedar(104) quedo(7) pensar(7) gente(5) querer(5) \n",
      "Cluster 49: re(163) puta_madre(8) loco(8) ver(8) ah(7) \n",
      "Cluster 50: ⠀(685) the(69) to(56) and(36) of(32) \n",
      "Cluster 51: paso(117) pasar(88) paz(10) pase(9) pan(8) \n",
      "Cluster 52: milei(180) espert(121) caño(43) debate(33) él(19) \n",
      "Cluster 53: tener(217) tenés(54) año(15) tenia(14) él(11) \n",
      "Cluster 54: lado(35) pelado(15) pedo(12) grado(10) armado(9) \n",
      "Cluster 55: entender(106) vender(99) aprender(52) depender(29) gente(20) \n",
      "Cluster 56: votar(127) voto(115) perder(40) peronismo(29) ganar(24) \n",
      "Cluster 57: ganar(84) laburo(73) semana(67) mes(61) pasar(56) \n",
      "Cluster 58: decir(193) favor(7) pensar(6) decis(5) dej(5) \n",
      "Cluster 59: acá(142) pobre(13) aca(13) argentina(7) sub(7) \n",
      "Cluster 60: leer(149) ley(148) post(44) comentario(34) sub(27) \n",
      "Cluster 61: peronismo(104) peronista(97) k(59) votar(52) milei(51) \n",
      "Cluster 62: tipo(281) tiro(39) tirar(22) pobre(21) capaz(15) \n",
      "Cluster 63: país(317) argentina(18) gente(17) mundo(16) pasar(16) \n",
      "Cluster 64: precio(219) controlar(21) comprar(19) control(19) producto(18) \n",
      "Cluster 65: él(956) hacer(59) ver(45) querer(39) gente(38) \n",
      "Cluster 66: año(381) pasar(20) venir(19) pensar(14) esperar(13) \n",
      "Cluster 67: vo(55) favor(3) decir(3) gracias(3) favor_robo(2) \n",
      "Cluster 68: gobierno(195) nacional(32) terrorista(23) elección(18) inflación(13) \n",
      "Cluster 69: ah(127) macri(14) pacer(6) cu(6) decir(5) \n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_frequent = Counter(\" \".join(df_clusters.query(f\"cluster == {i}\")[\"tokens\"]).split()).most_common(5)\n",
    "    for t in most_frequent:\n",
    "        tokens_per_cluster += f\"{t[0]}({str(t[1])}) \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupere los documentos más representativos (basados en los centroides de los clústeres) para un cluster en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No peronistas: Yrigoyen (1916 - 1922), Alvear y Macri.. Peronistas: Perón (1946 - 1952), Menem, Kirchner y CFK.. &#x200B;. Antes del 2000, la diferencia no era tan grande.\n",
      "-------------\n",
      "Macri=bad Cristina=good los pasacalles de mayra dicen vota la lista de Cristina y al bigote que tenemos de presidente lo limpiaron de todos los afiches.\n",
      "-------------\n",
      "Es una catarata infinita de falacias y sesgos, podría escribir un libro comentado lo objetable. Sigue siendo mejor que un talibán kirchnerista pero .. me hace acordar mucho a macri, kirchnerismo con buenos modales\n",
      "-------------\n",
      "La oposición conformada por Fernanda Vallejos, El Gato Silvestre, y liderado por la mismísima Cristina F Kirchner\n",
      "-------------\n",
      "No entiendo qué piensan cuando reciben noticias como lo de la plata enterrada de Máximo, los dólares traídos de contrabando para campaña de Antonini Wilson, la rosadita y todas esas cosas ya probadas ¿Solo ah, pero Macri?\n",
      "-------------\n",
      "Qué es lo q festejan? Por qué sonríen, cantan y alaban a los políticos? Acaso no les cabe la posibilidad de que Cristina y Néstor sean responsables de estado del país, también? Todo lo malo es solo por Macri?\n",
      "-------------\n",
      "Falto Lazaro Baez/Nestor Kirchner en esa infografia.\n",
      "-------------\n",
      "[[C1-FECHA 4] Mc Cree como Acuario (?](http://imgur.com/gallery/8mBx4W8). he descubierto que con Macri somos del mismo signo (?. espero que les guste :3. ~~u/unLucas agarrate catalina porque hoy me llevo el 1° puesto (?~~\n",
      "-------------\n",
      "Ahhh mi buen representante sindical.... CALO TE BAJASTE LOS LOMPA CON LA CHORRA DE CRISTINA 2 AÑOS SEGUIDOS.. Disculpen el exabrupto, esta persona habla de adoctrinar. Por eso el pais esta como esta, por que adoctrinaron al 35%.\n",
      "-------------\n",
      "En formorsa creo. Estan las estatuas de albert, maradona y los peronchos.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 0\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "for d in most_representative_docs[:10]:\n",
    "    print( df[\"body\"].values[d])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(len(vectorized_docs))\n",
    "#print(vectorized_docs[0])\n",
    "\n",
    "test_v = vectorize([['defender', 'peso', 'siente', 'corazón', 'compro', 'pesos', 'tasa', 'fijo', 'año']], model=model)\n",
    "prediction = clustering.predict(test_v)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(TEXT_FILE_READ)\n",
    "\n",
    "def get_cluster(row):\n",
    "    test_v = vectorize([row], model=model)\n",
    "    return clustering.predict(test_v)\n",
    "\n",
    "reddit['cluster'] = reddit.apply(lambda row: get_cluster(row['lemma_tokens']) , axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>is_replay</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>lemma_tokens</th>\n",
       "      <th>body_preprocessing</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw14mt</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>1</td>\n",
       "      <td>todo para decir que tapaste el baño. tira un b...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tapastir', 'baño', 'tirar', 'balde', 'aguo']</td>\n",
       "      <td>tapastir baño tirar balde aguo</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw41eh</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>0</td>\n",
       "      <td>sopapa primero master, si hay tapón te vas a t...</td>\n",
       "      <td>hfw14mt</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sopapa', 'master', 'tapón', 'va', 'teñir', '...</td>\n",
       "      <td>sopapa master tapón va teñir medio</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw1ao2</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>0</td>\n",
       "      <td>Usas la sopapa, o tiras agua caliente con un b...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sopapo', 'tira', 'agua', 'caliente', 'balde']</td>\n",
       "      <td>sopapo tira agua caliente balde</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw3jof</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>2</td>\n",
       "      <td>Lo que he probado que siempre me dio resultado...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['probado', 'resultado', 'sellar', 'boca', 'in...</td>\n",
       "      <td>probado resultado sellar boca inodoro tirar ca...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw6v4i</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>0</td>\n",
       "      <td>Estas cobrando por dar mantenimiento y no sabe...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['cobrar', 'mantenimiento', 'carajo', 'kjjjjjj...</td>\n",
       "      <td>cobrar mantenimiento carajo kjjjjjjjjj vivirio...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw26iv</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>0</td>\n",
       "      <td>Si tenes algo con punta, metelo y hace un poco...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tén', 'punto', 'metelo', 'fuerza', 'romper',...</td>\n",
       "      <td>tén punto metelo fuerza romper tapo baño tirar...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw2gof</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>1</td>\n",
       "      <td>Con una manguera para regar el jardín, si tene...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['regar', 'jardín', 'tén', 'pod', 'probar']</td>\n",
       "      <td>regar jardín tén pod probar</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw5s13</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>0</td>\n",
       "      <td>despues regas el jardin y se lava sola, solo q...</td>\n",
       "      <td>hfw2gof</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['rega', 'jardin', 'lava', 'tenés', 'lavarte',...</td>\n",
       "      <td>rega jardin lava tenés lavarte mano pulgar chorro</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw3air</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>0</td>\n",
       "      <td>La respuesta real es que se venden unos caños ...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['respuesta', 'real', 'vender', 'caño', 'alamb...</td>\n",
       "      <td>respuesta real vender caño alambrado decir cañ...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>hfvxa6w</td>\n",
       "      <td>Discusion🧐</td>\n",
       "      <td>3</td>\n",
       "      <td>Mi alfajor favorito es el Havana</td>\n",
       "      <td>q443eo</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['alfajor', 'favorito', 'haván']</td>\n",
       "      <td>alfajor favorito haván</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score       id       flair  comms_num  \\\n",
       "0      1  hfw14mt  Discusion🧐          1   \n",
       "1      1  hfw41eh  Discusion🧐          0   \n",
       "2      1  hfw1ao2  Discusion🧐          0   \n",
       "3      1  hfw3jof  Discusion🧐          2   \n",
       "4      1  hfw6v4i  Discusion🧐          0   \n",
       "5      1  hfw26iv  Discusion🧐          0   \n",
       "6      1  hfw2gof  Discusion🧐          1   \n",
       "7      1  hfw5s13  Discusion🧐          0   \n",
       "8      1  hfw3air  Discusion🧐          0   \n",
       "9      7  hfvxa6w  Discusion🧐          3   \n",
       "\n",
       "                                                body comment_parent_id  \\\n",
       "0  todo para decir que tapaste el baño. tira un b...            q44kw3   \n",
       "1  sopapa primero master, si hay tapón te vas a t...           hfw14mt   \n",
       "2  Usas la sopapa, o tiras agua caliente con un b...            q44kw3   \n",
       "3  Lo que he probado que siempre me dio resultado...            q44kw3   \n",
       "4  Estas cobrando por dar mantenimiento y no sabe...            q44kw3   \n",
       "5  Si tenes algo con punta, metelo y hace un poco...            q44kw3   \n",
       "6  Con una manguera para regar el jardín, si tene...            q44kw3   \n",
       "7  despues regas el jardin y se lava sola, solo q...           hfw2gof   \n",
       "8  La respuesta real es que se venden unos caños ...            q44kw3   \n",
       "9                   Mi alfajor favorito es el Havana            q443eo   \n",
       "\n",
       "  is_replay Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11  \\\n",
       "0     False        NaN        NaN        NaN         NaN         NaN   \n",
       "1      True        NaN        NaN        NaN         NaN         NaN   \n",
       "2     False        NaN        NaN        NaN         NaN         NaN   \n",
       "3     False        NaN        NaN        NaN         NaN         NaN   \n",
       "4     False        NaN        NaN        NaN         NaN         NaN   \n",
       "5     False        NaN        NaN        NaN         NaN         NaN   \n",
       "6     False        NaN        NaN        NaN         NaN         NaN   \n",
       "7      True        NaN        NaN        NaN         NaN         NaN   \n",
       "8     False        NaN        NaN        NaN         NaN         NaN   \n",
       "9     False        NaN        NaN        NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 12 Unnamed: 13 Unnamed: 14  \\\n",
       "0         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN   \n",
       "5         NaN         NaN         NaN   \n",
       "6         NaN         NaN         NaN   \n",
       "7         NaN         NaN         NaN   \n",
       "8         NaN         NaN         NaN   \n",
       "9         NaN         NaN         NaN   \n",
       "\n",
       "                                        lemma_tokens  \\\n",
       "0     ['tapastir', 'baño', 'tirar', 'balde', 'aguo']   \n",
       "1  ['sopapa', 'master', 'tapón', 'va', 'teñir', '...   \n",
       "2    ['sopapo', 'tira', 'agua', 'caliente', 'balde']   \n",
       "3  ['probado', 'resultado', 'sellar', 'boca', 'in...   \n",
       "4  ['cobrar', 'mantenimiento', 'carajo', 'kjjjjjj...   \n",
       "5  ['tén', 'punto', 'metelo', 'fuerza', 'romper',...   \n",
       "6        ['regar', 'jardín', 'tén', 'pod', 'probar']   \n",
       "7  ['rega', 'jardin', 'lava', 'tenés', 'lavarte',...   \n",
       "8  ['respuesta', 'real', 'vender', 'caño', 'alamb...   \n",
       "9                   ['alfajor', 'favorito', 'haván']   \n",
       "\n",
       "                                  body_preprocessing cluster  \n",
       "0                     tapastir baño tirar balde aguo    [18]  \n",
       "1                 sopapa master tapón va teñir medio    [18]  \n",
       "2                    sopapo tira agua caliente balde    [18]  \n",
       "3  probado resultado sellar boca inodoro tirar ca...    [18]  \n",
       "4  cobrar mantenimiento carajo kjjjjjjjjj vivirio...    [18]  \n",
       "5  tén punto metelo fuerza romper tapo baño tirar...    [18]  \n",
       "6                        regar jardín tén pod probar    [18]  \n",
       "7  rega jardin lava tenés lavarte mano pulgar chorro    [18]  \n",
       "8  respuesta real vender caño alambrado decir cañ...    [18]  \n",
       "9                             alfajor favorito haván    [18]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show\n",
    "reddit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv(TEXT_SAVE_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docs/testfasttext/0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-1823d2318032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mreddit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flair'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'docs/testfasttext/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docs/testfasttext/0.csv'"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    reddit[(reddit[\"cluster\"] == i)][['flair', 'body']].to_csv('docs/testfasttext/' + str(i) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
