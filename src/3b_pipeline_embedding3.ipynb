{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Neuronales\n",
    "\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importanción de librería requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE_READ = 'docs/preprocessing_reddit_data.csv'\n",
    "TEXT_SAVE_FILE = 'docs/reddit_data_lda.csv'\n",
    "FILENAME_PICKLE = \"docs/tmpreddit.pickle\"\n",
    "n_clusters = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los comentarios de Reddit\n",
    "\n",
    "Los comentarios fueron previamente preprocesados (Ver en TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(FILENAME_PICKLE, 'rb') as f:\n",
    "    df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['lemma_tokens'])\n",
    "\n",
    "# Filtering Extremes\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "\n",
    "# Creating a corpus object\n",
    "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = df['lemma_tokens']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(processed_corpus, total_examples=len(processed_corpus), epochs=100)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = []\n",
    "vocabulary = list(model.wv.key_to_index)\n",
    "\n",
    "for key in model.wv.key_to_index:\n",
    "    word_vecs.append(model.wv[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jajajajjajaajaj', 0.9570168256759644),\n",
       " ('bottle', 0.9228257536888123),\n",
       " ('laconcho', 0.9012131094932556),\n",
       " ('branding', 0.8885334134101868),\n",
       " ('submarinar', 0.8106595873832703),\n",
       " ('meanies', 0.7711156010627747),\n",
       " ('golden', 0.7673962116241455),\n",
       " ('frondizi', 0.6835584044456482),\n",
       " ('gustavo', 0.676913321018219),\n",
       " ('oct', 0.6762269139289856)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algunas predicciones\n",
    "\n",
    "model.wv.most_similar(\"rucula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los clústers\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "X_wvkm = kmeans.fit_transform(word_vecs)\n",
    "y_wvkm = kmeans.predict(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(n_clusters):\n",
    "    mask = X_wvkm[y_wvkm == cluster]\n",
    "    idx_sort = np.argsort(X_wvkm[:,cluster])\n",
    "    words = [vocabulary[x] for x in idx_sort[:20]]\n",
    "\n",
    "    print(\"Clúster %d:\" % cluster, end='')\n",
    "    print()\n",
    "    for w in words:\n",
    "        print(' %s' % w, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-8e64c508c0ab>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Nothing is easy in cricket. Maybe when you watch it on TV, it looks easy. But it is not. You have to use your brain and time the ball.\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mpredicted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredicted\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Word2Vec' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "X = model.transform([\"Nothing is easy in cricket. Maybe when you watch it on TV, it looks easy. But it is not. You have to use your brain and time the ball.\"])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'peron k'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mValueError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-adecac7fad74>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#reddit = pd.read_csv(TEXT_FILE_READ)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkmeans\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'peron k'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, X, sample_weight)\u001B[0m\n\u001B[1;32m   1154\u001B[0m         \u001B[0mcheck_is_fitted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1155\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1156\u001B[0;31m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_test_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1157\u001B[0m         \u001B[0mx_squared_norms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrow_norms\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msquared\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1158\u001B[0m         \u001B[0msample_weight\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_sample_weight\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001B[0m in \u001B[0;36m_check_test_data\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    857\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_check_test_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 858\u001B[0;31m         X = self._validate_data(X, accept_sparse='csr', reset=False,\n\u001B[0m\u001B[1;32m    859\u001B[0m                                 \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    860\u001B[0m                                 order='C', accept_large_sparse=False)\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    419\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    420\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'no_validation'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 421\u001B[0;31m             \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    422\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    423\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    614\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcasting\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"unsafe\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    615\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 616\u001B[0;31m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    617\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    618\u001B[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/numpy/core/_asarray.py\u001B[0m in \u001B[0;36masarray\u001B[0;34m(a, dtype, order, like)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_asarray_with_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlike\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlike\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'peron k'"
     ]
    }
   ],
   "source": [
    "#reddit = pd.read_csv(TEXT_FILE_READ)\n",
    "\n",
    "test = kmeans.predict(['peron k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mNameError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-3bc6909a44d2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;32mreturn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent_topics_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0mdf_topic_sents_keywords\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformat_topics_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mldamodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbase_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtexts\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreddit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv(TEXT_FILE_READ)\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # row = sorted(row, key=lambda x: (x[1]), reverse=True) # old line\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                #ent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4)]), ignore_index=True)\n",
    "                #print(sent_topics_df)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    #sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    #contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, texts], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=base_model, corpus=corpus, texts=reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>is_replay</th>\n",
       "      <th>lemma_tokens</th>\n",
       "      <th>body_preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>él, recordar, pegar, único, robar, barrio, pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw77qe</td>\n",
       "      <td>Política🏛️</td>\n",
       "      <td>0</td>\n",
       "      <td>Iba a decir, bue si lo saco de su bolsillo... ...</td>\n",
       "      <td>q9imco</td>\n",
       "      <td>False</td>\n",
       "      <td>['bue', 'saco', 'bolsillo', 'recorder', 'hdp',...</td>\n",
       "      <td>bue saco bolsillo recorder hdp mantener alcanz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2598</td>\n",
       "      <td>perro, nik, meme, gobierno, explicar, it, teni...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw7dci</td>\n",
       "      <td>Política🏛️</td>\n",
       "      <td>0</td>\n",
       "      <td>Se volvio un meme el bot del dolar?</td>\n",
       "      <td>hgw666m</td>\n",
       "      <td>True</td>\n",
       "      <td>['volvio', 'meme', 'dolar']</td>\n",
       "      <td>volvio meme dolar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.3279</td>\n",
       "      <td>falacia, decir, gratis, k, joda, país, mandar,...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw69er</td>\n",
       "      <td>Humor:snoo_joy:</td>\n",
       "      <td>0</td>\n",
       "      <td>Este Esteban Lamothe estaba en la ficción de u...</td>\n",
       "      <td>q9i4uj</td>\n",
       "      <td>False</td>\n",
       "      <td>['ester', 'lamothe', 'ficción', 'villo', 'acá'...</td>\n",
       "      <td>ester lamothe ficción villo acá comedia políti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4349</td>\n",
       "      <td>pobre, servir, él, comida, tenés, culpa, onda,...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw6zvd</td>\n",
       "      <td>Meme💩</td>\n",
       "      <td>0</td>\n",
       "      <td>Eso porque son todos útos chupa bija.. Venga e...</td>\n",
       "      <td>hgw2528</td>\n",
       "      <td>True</td>\n",
       "      <td>['úto', 'chupa', 'bijo', 'venir', 'ban', 'nedf...</td>\n",
       "      <td>úto chupa bijo venir ban nedflanducacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.8089</td>\n",
       "      <td>re, cabeza, él, morir, pibes, papa, hambre, ri...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw24ns</td>\n",
       "      <td>Meme💩</td>\n",
       "      <td>0</td>\n",
       "      <td>mas verso burgués que Maximo no hay. Es la rep...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['verso', 'burgués', 'maximo', 'representación']</td>\n",
       "      <td>verso burgués maximo representación</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.3722</td>\n",
       "      <td>él, recordar, pegar, único, robar, barrio, pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw38x8</td>\n",
       "      <td>Meme💩</td>\n",
       "      <td>0</td>\n",
       "      <td>Ayudar con comida? Na mejor unos afiches a tod...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['ayudar', 'comida', 'na', 'afich', 'color']</td>\n",
       "      <td>ayudar comida na afich color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.3461</td>\n",
       "      <td>the, of, necesitar, you, fácil, and, to, creer...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw2rml</td>\n",
       "      <td>Meme💩</td>\n",
       "      <td>1</td>\n",
       "      <td>¿Por qué si es un cerdo tiene 6 patas?</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['cerdo', 'pata']</td>\n",
       "      <td>cerdo pata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>ah, /s, peronista, paso, x200b, mes, cagar, él...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw3wei</td>\n",
       "      <td>Meme💩</td>\n",
       "      <td>0</td>\n",
       "      <td>Mira, soy tan capitalista que por 15 mil pesos...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['mira', 'capitalisto', 'pesos', 'corrijo', 'c...</td>\n",
       "      <td>mira capitalisto pesos corrijo color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.4082</td>\n",
       "      <td>milei, pasar, debate, votar, voto, mujer, izqu...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw78bv</td>\n",
       "      <td>Meme💩</td>\n",
       "      <td>0</td>\n",
       "      <td>Swinetaur libertario de Darkest Perónia. Ruin ...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['swinetaur', 'libertario', 'darkest', 'peróni...</td>\n",
       "      <td>swinetaur libertario darkest perónia ruin come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.4081</td>\n",
       "      <td>él, foto, ver, libertad, sacar, feriado, tomar...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw6rim</td>\n",
       "      <td>Meme💩</td>\n",
       "      <td>0</td>\n",
       "      <td>como no pueden contra elllll. lo ensucian vamo...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['elllll', 'ensuciar', 'milie', 'bastar', 'k']</td>\n",
       "      <td>elllll ensuciar milie bastar k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Dominant_Topic  Perc_Contribution  \\\n",
       "0      0            12.0             0.2531   \n",
       "1      1             2.0             0.2598   \n",
       "2      2            26.0             0.3279   \n",
       "3      3             6.0             0.4349   \n",
       "4      4            21.0             0.8089   \n",
       "5      5            12.0             0.3722   \n",
       "6      6            17.0             0.3461   \n",
       "7      7            22.0             0.3515   \n",
       "8      8            24.0             0.4082   \n",
       "9      9            23.0             0.4081   \n",
       "\n",
       "                                      Topic_Keywords  score       id  \\\n",
       "0  él, recordar, pegar, único, robar, barrio, pen...      1  hgw77qe   \n",
       "1  perro, nik, meme, gobierno, explicar, it, teni...      1  hgw7dci   \n",
       "2  falacia, decir, gratis, k, joda, país, mandar,...      1  hgw69er   \n",
       "3  pobre, servir, él, comida, tenés, culpa, onda,...      1  hgw6zvd   \n",
       "4  re, cabeza, él, morir, pibes, papa, hambre, ri...      1  hgw24ns   \n",
       "5  él, recordar, pegar, único, robar, barrio, pen...      1  hgw38x8   \n",
       "6  the, of, necesitar, you, fácil, and, to, creer...      1  hgw2rml   \n",
       "7  ah, /s, peronista, paso, x200b, mes, cagar, él...      1  hgw3wei   \n",
       "8  milei, pasar, debate, votar, voto, mujer, izqu...      1  hgw78bv   \n",
       "9  él, foto, ver, libertad, sacar, feriado, tomar...      1  hgw6rim   \n",
       "\n",
       "             flair  comms_num  \\\n",
       "0       Política🏛️          0   \n",
       "1       Política🏛️          0   \n",
       "2  Humor:snoo_joy:          0   \n",
       "3            Meme💩          0   \n",
       "4            Meme💩          0   \n",
       "5            Meme💩          0   \n",
       "6            Meme💩          1   \n",
       "7            Meme💩          0   \n",
       "8            Meme💩          0   \n",
       "9            Meme💩          0   \n",
       "\n",
       "                                                body comment_parent_id  \\\n",
       "0  Iba a decir, bue si lo saco de su bolsillo... ...            q9imco   \n",
       "1                Se volvio un meme el bot del dolar?           hgw666m   \n",
       "2  Este Esteban Lamothe estaba en la ficción de u...            q9i4uj   \n",
       "3  Eso porque son todos útos chupa bija.. Venga e...           hgw2528   \n",
       "4  mas verso burgués que Maximo no hay. Es la rep...            q9hut7   \n",
       "5  Ayudar con comida? Na mejor unos afiches a tod...            q9hut7   \n",
       "6             ¿Por qué si es un cerdo tiene 6 patas?            q9hut7   \n",
       "7  Mira, soy tan capitalista que por 15 mil pesos...            q9hut7   \n",
       "8  Swinetaur libertario de Darkest Perónia. Ruin ...            q9hut7   \n",
       "9  como no pueden contra elllll. lo ensucian vamo...            q9hut7   \n",
       "\n",
       "   is_replay                                       lemma_tokens  \\\n",
       "0      False  ['bue', 'saco', 'bolsillo', 'recorder', 'hdp',...   \n",
       "1       True                        ['volvio', 'meme', 'dolar']   \n",
       "2      False  ['ester', 'lamothe', 'ficción', 'villo', 'acá'...   \n",
       "3       True  ['úto', 'chupa', 'bijo', 'venir', 'ban', 'nedf...   \n",
       "4      False   ['verso', 'burgués', 'maximo', 'representación']   \n",
       "5      False       ['ayudar', 'comida', 'na', 'afich', 'color']   \n",
       "6      False                                  ['cerdo', 'pata']   \n",
       "7      False  ['mira', 'capitalisto', 'pesos', 'corrijo', 'c...   \n",
       "8      False  ['swinetaur', 'libertario', 'darkest', 'peróni...   \n",
       "9      False     ['elllll', 'ensuciar', 'milie', 'bastar', 'k']   \n",
       "\n",
       "                                  body_preprocessing  \n",
       "0  bue saco bolsillo recorder hdp mantener alcanz...  \n",
       "1                                  volvio meme dolar  \n",
       "2  ester lamothe ficción villo acá comedia políti...  \n",
       "3           úto chupa bijo venir ban nedflanducacion  \n",
       "4                verso burgués maximo representación  \n",
       "5                       ayudar comida na afich color  \n",
       "6                                         cerdo pata  \n",
       "7               mira capitalisto pesos corrijo color  \n",
       "8  swinetaur libertario darkest perónia ruin come...  \n",
       "9                     elllll ensuciar milie bastar k  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "#df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(TEXT_SAVE_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}